HOW TO USE
----------

This is the Windows version of the V1 software, which is general and should work with any source without training.  

You will need the Python module 'Colorama' for this.  This displays the ANSI colours generated by the Linux branch
into Windows console colours.

You will need to sample a VBI to use this; the target card, the Hauppauge WinTV PCI (date late 90s) driver doesn't 
support Windows x64 so you will probably need to use Linux to capture the teletext signal.  The original, Linux 
instructions are below - use these to get the .vbi files, then you can copy these to Windows and process them there 
using these files.

You can use 'vbicat' from the v2 branch to capture files; this version of vbi.py will happily work with the 64K VBI files 
or the monolithic VBI files from v2's vbicat.  However threading is not supported using the monolithic file.

To use:

1. 	Boot into Linux and fetch a VBI file using vbicat (see below).

	To see what you have before you sample a big file which doesn't recover well, use the Bash scripts 'newgrab.sh' 
	to capture data (CTRL-C after a couple of seconds),then 'process.sh' to process them - the processing will 
	display headers as it goes so you can see what the date was and gauge the recovery quality.  You will probably 
	have to make these files 'executable' by right-clicking on them and choosing 'properties' (or doing a chmod if
	you're that way inclined).

2. 	Copy the single file generated by vbicat into a drive accessible to Windows.

3. 	Boot into Windows.

4.	Open the 'template' folder and copy the contents to the root of an empty USB stick, for example mounted as g: .
	Using a USB stick for this reduces the wear on your hard disk by saving it thrashing about with small files.  USB 
	drives are expendable.
	
5. 	Use splitvbi.py to split the monolithic VBI file into lots of 64K VBI files.    
	Example command: 
	
		splitvbi.py test.vbi g:\vbi
		
6.	Whilst it is splitting the file, you can start recovery by typing:

		vbi.py g:
		
	This will process the vbi files and turn them into teletext data in the g:\t42 folder.  It shows you headers as it 
	goes, and will also highlight possible Level 2 packets.
	
	Any packets that are > X/24 will be written to g:\datapackets.txt.  These packets are copied as-is in this version 
	and are not subject to odd-parity checks etc. as they were in the original branch.
	
7.	When finished (or during processing if you like) you can run:

		pagesplit.py g:
	
	This will sort the t42 files into magazines and pages in the g:\pages folder.  It writes a file called 
	g:\packetorder.txt which lists the packets in the order that they were broadcast.  (This incorporates t42pipe from 
	the Linux version so you don't have to muck about with that step).
	
	This version of pagesplit.py will work with 42 byte t42 files or monolithic t42 files like those generated by the v2 branch.
	
8.	After pagesplit.py has been run you can execute:

		subpagesquash.py g:
		
	This generates HTML versions of the recovered service in the g:\html folder.  There are two additional switches over the 
	original version - 'nosquash' doesn't squash any of the pages, useful for a small capture.  'nosquash777' squashes all 
	except page 777, useful for keeping all pages nicely squashed but leaving 777, which was the Televox page on ITV, 
	unsquashed so you can see what callers were up to.
	
	It also dumps the raw teletext data into 
	g:\binaries - this folder can be dragged into the main window of the Teletext Recovery Editor here: 
	https://teletextarchaeologist.org/teletextrecoveryeditor where you can look at the files and correct recovery errors.
	
9.	Run copy /b g:\vbi [folder_on_your_hard_disk]\filename.t42 to concatenate all of the 42 byte packets into one,
	monolithic t42 file.  Again, the one big file reduces wear-and-tear on your hard disk and you can drop it into the
	Teletext Player from here: http://teletextarchaeologist.org/teletextplayer to play the file back.  This allows you to
	see the recovered service ***exactly as it was broadcast***.  It has loads of awesome features to play with too.
	
10.	Copy all from g: except the t42 and vbi folders to your hard disk.  You should already have the monolithic VBI and T42 
	files.  VBI files compress quite well, so do that to save space.
		
	
Below are some Linux instructions, adapted from the original V1 README.  You will need to do this on a Linux machine with 
a suitable WinTV PCI card - the late 90s ones seem OK, but they should output 2048 bytes per line and 32 lines per frame. You
can't know this without trying it, but at least the cards have a date printed on a sticker on the card's RF box.

You'll need to install some packages:

	sudo apt-get install python-numpy python-scipy tv-fonts xterm git

Then checkout source and run on the example data like this:

	git clone git://github.com/ali1234/vhs-teletext.git

Manually copy in vbicat from this branch.  Then run:

	./vbicat /dev/vbi0 > bbc1_1987-02-04.vbi
	
...this will sample the teletext data to the file 'bbc1_1987-02-04.vbi' - this is the file you copy to Windows and 
the split using splitvbi.py.





HOW IT WORKS
------------

Teletext is encoded as a non-return-to-zero signal with two levels representing
one and zero. This is a fancy way of saying that a line of teletext data is
a sequence of black and white "pixels" in the TV signal. Of course, since the
signal is analogue there are no individual pixels, the signal is continuous.
But you can imagine that there are pixels in the idealized "perfect" signal.

The problem of decoding teletext from a VHS recording is that VHS bandwidth is
lower than teletext bandwidth. This means that the signal is effectively low
pass filtered, which in terms of an image is equivalent to gaussian blurring.

There are methods for reversing gaussian blur, but they are designed to work
with general image data. In the case of teletext we only have black or white
levels, so these methods are not optimal. We can exploit the limitations on
the input in order to get a better result. We can also exploit information
about the protocol to further improve efficiency and accuracy.

When the black and white signal is blurred, the individual pixels are blurred 
in to each other. This makes the signal unreadable using normal methods, because
instead of a clean sequence like "1010" you something close to "0.5 0.5 0.5 0.5".
But all is not lost, because a sequence like "1111" or "0000" will be the same
after blurring. So if you see a signal like "0.5 0.7 1.0 1.0" you can guess that
the original was probably "0 1 1 1" or "0 0 1 1".  

What this software does is take the sampled blurred signal and tries to guess
what original data would produce the sampled result. It does this by applying
a guassian filter approximating the VHS encoding to the guess, and then testing
the result against the observed data.

There are 45 bytes in each teletext line, so the space of possible guesses is
2^(45*8) which is a very big number, which makes trying every guess completely
impractical. However there are ways to reduce this number:

FOUR RULES
----------

1. Nearly all bytes have a parity bit which means there are only 128 possible
combinations instead of 256.

2. Some bytes are hamming encoded. These have even fewer possible combinations.

3. The first three bytes in the signal are always the same. We can use this
to find the start of the signal in the sample data (it moves a bit in each
line, but the width is always the same.)

4. The protocol itself defines rules about which bytes are allowed in which
positions, reducing the problem space further.


GUESSING ALGORITHM
------------------

1. We start with a completely empty guess, all bits are unknown.

2. We fill in the first three known bytes, and apply the gaussian filter,
and then correlate the result with the samples to find the beginning of
the signal.

3. Take the samples input and find areas which are close to 0 or 1. These
represent a run of 0s or 1s in the original data, so we fix these values
in the guess. This typically fills in about 5-10 of the original bits.

4. Next we have to guess what all the other values are. This is done recursivly,
one byte at a time, from the beginning of the signal to the end. Like so:

a. For the current byte position, apply the FOUR RULES to restrict the number
of guesses. From the resulting set of possible bytes, remove any that contradict
bits we filled in during step 3.

b. Try each possible byte guess in sequence and find the one that most closely
matches the samples.

c. Fill the best result in to the guess and go to the next byte.

4. During the first pass, the subsequent bytes after the one we are currently
guessing are completely wrong. So we go back to the start of the signal and
try every possible byte again, but this time we have a better approximation in
the subsequent bytes.

We keep repeating this until the guess converges - ie it doesn't change
at all during a pass. This usually takes about 5 - 10 passes.

Once the guess has converged, this is the decoded teletext bit data. This is
written out to a file for more processing later.

It should be noted that due to only trying possible bytes in the guess, the first
stage always outputs valid teletext packets with correct parity etc, so we can't
do further error detection, since no parity or hamming errors can ever be produced.


LINE CLEANUP
------------

The guessing algorithm output lots of teletext packets, but they will still not be    
perfect (even though they are valid, they aren't necessarily correct.)

Since the teletext pages are broadcast on a loop, any recording of more than a
few minutes will have multiple copies of every packet. A hamming distance check
it used to determine if any two output guesses are probably the same line.

This means, if two packets are received that only differ at a couple of bytes,
they are assumed to be the same.

All versions of the same packet are compared, and for each byte, the most frequent
decoding is used so for example if you had these inputs:

HELLO
HELLP
MELLO

Then the result "HELLO" would be decoded, since those are the most frequent bytes
in this position. For this to work well, you need a lot of copies of every packet.
This procedure only considers the visible data, not row numbers, because the same
packet may appear on many pages if it is a repeated header graphic for example.

Also whitespace is ignored, because many packets are all whitespace except for eg
a subpage number, and these should not be combined. In order to match, a packet
must have more than X non-whitespace characters the same, and less the Y different.

After applying this check, each packet in the input is replaced by it's "cleaned"
version in the output, because sequence of packets is important.


FINDING HEADERS
---------------

The guessing algorithm output lots of teletext lines, but they will still not be
perfect. We need to rebuild it into pages.

Each teletext page starts off with a special header called packet zero with a
special format. This format is considered as part of the FOUR RULES above, so
such headers are more likely to be decoded correctly. The headers usually have a
very specific format containing the broadcaster ident and a clock, which means
that the "possible bytes" can be highly refined eg "last byte is always a digit".

A teletext page is defined as all the packets which follow the header, until the
next header. This is why packet sequence is preserved at the previous step.

The rules for headers are defined in finders.py and if you want to decode for
a new broadcaster you need to define a finder for it. Do this by running the
previous stages first, then examining the output. The headers will not be detected
but they will still be output like normal packets and you should be able to
reconstruct a pristine one by hand (approx 4% of all lines are headers so you
should find plenty of examples.)

After you program in the rules for the header packet, the software will find
them much more easily and with higher accuracy.


PAGE REBUILD
------------

The data is considered in sequence and for each header, subsequent packets with
the same magazine number are considered part of the page.

Once anew header is received, all the previously received packets are passed through
a sanity check. If most of the lines are present (1-26 or so) then the page is
considered complete and written out to disk. If many lines are missing then the page is
discarded as unrecoverable.

If you have a lot of input data (and you should) then you will get multiple examples
of each page. They are all combined again using hamming distance like was done with
individual packets, with the result being considered the best version of the page.

There are also rules which recognize graphical elements in pages such as logos. For
better results you can teach new rules about these, which will improve decodes.
They are in fragment.py and they work kind of like finders, but in 2 dimensions.


FINAL NOTES
-----------

For best results the software needs to be taught lots of things about the
data you are decoding:

* Broadcaster header format (finders.py)
* Common logos (fragment.py)

It also needs a large amount of input data due to discarding lots of bad, 
unrecoverable parts. You need up to half an hour of real-time sampled data in 
order to get best results. Unfortunately the decoder doesn't run in anything
like real-time, and this might take a month to do the first stage guess filtering
on a quad core PC.

The software expects the sampled lines to be 2048 8-bit samples. This is what
bt8x8 produces. Other capture hardware produces 1440 samples. In this case,
you need to resample to 2048 samples when loading the data.
